{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle\n",
    "\n",
    "\n",
    "## Gather\n",
    "Data files were gathered from three sources:\n",
    "1. Data-enhanced.csv which was provided\n",
    "2. Data-predictions.csv which was downloaded programmatically \n",
    "3. Data-additional.txt which was created by saving response from querying twitter API for tweet IDs\n",
    "Relevant data from these files were then loaded into dataFrames. \n",
    "\n",
    "## Assess\n",
    "\n",
    "### Tidiness\n",
    "\n",
    "#### Each type of observational unit forms a table:\n",
    "\n",
    "The observational unit in this instance was information pertaining to one tweet ID (or one dog.) \n",
    "Merging enhanced and additional to this tidiness requirement was intuitive as every tweet_id gathered for additional was equivalent the tweet IDs included in enhanced. Every row in predictions relates to one tweet ID (or one dog) so this was also merged to create one master dataFrame. \n",
    "\n",
    "#### Each variable forms a column:\n",
    "\n",
    "Information within the initial source column relaated to two variables, source_type and source_url. \n",
    "\n",
    "### Quality\n",
    "\n",
    "#### Completeness\n",
    "\n",
    "As it is not possible to run new tweet IDs through the predictions Neural Net, it was not possible to gather missing information - i.e. tweet IDs in enhanced not in predictions. \n",
    "\n",
    "#### Validity\n",
    "\n",
    "Was noticed name column was well provided but decided not to clean this as it didnâ€™t provided much use downstream relative to the effort of cleaning.\n",
    "\n",
    "Main issue relating to validity was many of the rating_denominators were not 10. The rating system is specifically out of 10.\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "The main issue with accuracy was around rating_numerator, one of the most interesting numerical data for analysis. There were numerous reasons for these inaccuracies including:\n",
    "* Groups of dogs being rated rather than individual dogs\n",
    "* Initial cleaning effort picking up other numerical information from within the text of the tweet\n",
    "* Missing ratings\n",
    "* Rating given as part of joke, so skewing data massively (almost all rating numerators in the 0-20 range)\n",
    "\n",
    "\n",
    "\n",
    "#### Consistency \n",
    "\n",
    "\n",
    "## Clean\n",
    "\n",
    "All data tidiness and quality issues noted in the assess stage were cleaned following the define, clean, test procedure in the following order:\n",
    "\n",
    "1. Quality - completeness\n",
    "2. Tidiness - all issues\n",
    "3. Quality - validity and accuracy\n",
    "4. Quality - consistency \n",
    "\n",
    "Validity and accuracy were tidied together as most of the issues here related to the same columns in the master dataFrame namely, rating_numerator and rating_denominator. \n",
    "\n",
    "Beautiful soup was used to access information from HTML tags\n",
    "\n",
    "Regular expressions were used for two purposes. One to access ratings numerators as original effort had not captured these correctly. Second to access source_url from within source column.\n",
    "\n",
    "All tests used assert statements.\n",
    "\n",
    "## General Comments\n",
    "\n",
    "Although ultimately cleaning in the order provided in the workbook, three iterations of the assess / clean steps were needed to complete the wrangling process as required. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:udacity_data_analyst_nanodegree]",
   "language": "python",
   "name": "conda-env-udacity_data_analyst_nanodegree-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
